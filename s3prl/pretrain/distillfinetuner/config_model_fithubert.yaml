distiller:
  # Extractor
  extractor_mode: default # default(hubert) or layer_norm(data2vec)
  extractor_conv_feature_layers: '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2'
  extractor_dropout: 0.0
  feature_grad_mult: 0.1

  # Convolutional relative positional encoding
  conv_pos: 128
  conv_pos_groups: 16

  # Transformer encoder
  encoder_layers: 12
  encoder_embed_dim: 480
  encoder_ffn_embed_dim: 480
  encoder_attention_heads: 12
  activation_fn: gelu
  layer_norm_first: false
  attention_type: original

  # Dropout
  dropout: 0.1
  attention_dropout: 0.1
  activation_dropout: 0.1
  encoder_layerdrop: 0.0

  # Output
  final_dim: 768
  out_layer_type: layer-wise
  teacher_final_dim: 768

  # Task & loss
  n_tasks: 3
  task_emb_type: layer-wise
  loss_type: l1
  feat_pen_loss: 0.0
  rec_loss: 1.0
  cosine_loss: 1.0  # cosine similarity loss
  hidden_loss: 0.0  # hidden loss
  attn_loss: 0.0 # attn loss
  kldiv_loss: 0.0 # kldiv loss
  temperature: 1.0 
  pred_layer_id: [1,2,3,4,5,6,7,8,9,10,11,12] # layers for predict head loss
  pred_layer_id_2: [4,8] # layers for hidden & attn loss

  # Initialization
  init_teacher_conv_layers: false
  init_teacher_encoder_layers: false
  get_hidden: false

  # decode
  dictionary_path: /mnt/lustre/sjtu/home/xc915/superb/dataset/librispeech_finetuning_data/100h/dict.ltr.txt
  enable_decode: True

teacher:
  model: hubert
  model_path: /mnt/lustre/sjtu/home/xc915/superb/upstream_model/fairseq-to-s3prl/hubert_finetune_to_pretrain.pt
  linear_projection_path: /mnt/lustre/sjtu/home/xc915/superb/upstream_model/linear_projection/hubert_linear_projection.pt
  n_layers: 12
  use_ckpt: False # student
  ckpt: /mnt/lustre/sjtu/home/xc915/superb/wyj-s3prl/s3prl/result/pretrain/distill_hubert_finetune/states-epoch-1.ckpt

task:
  sequence_length: 250000  # 15.6 secs

audio:
  target_level: None
